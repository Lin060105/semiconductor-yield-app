import streamlit as st
import pandas as pd
import numpy as np
import os
import shutil
from pycaret.classification import load_model, predict_model
from PIL import Image

# --- 1. é é¢è¨­å®š ---
st.set_page_config(
    page_title="Semiconductor Yield Prediction Pro",
    page_icon="ğŸ­",
    layout="wide"
)

st.title("ğŸ­ åŠå°é«”è‰¯ç‡é æ¸¬ç³»çµ± (v3.0 Ultimate)")
st.markdown("""
æœ¬ç³»çµ±åˆ©ç”¨ **CatBoost / Random Forest** æ•´åˆæ¨¡å‹é æ¸¬æ™¶ç‰‡è‰¯ç‡ã€‚
æ–°åŠŸèƒ½ï¼š**Fail Ranking** (å„ªå…ˆè™•ç†é«˜é¢¨éšªæ™¶ç‰‡) èˆ‡ **å®Œæ•´æ¨¡å‹è©•ä¼°å ±å‘Š**ã€‚
""")

# --- 2. è¼‰å…¥æ¨¡å‹èˆ‡è³‡æº ---
@st.cache_resource
def load_prediction_model():
    # å˜—è©¦å¤šå€‹è·¯å¾‘è¼‰å…¥æ¨¡å‹
    paths = [
        'output/final_yield_prediction_model', 
        'final_yield_prediction_model',
        'reports/final_yield_prediction_model'
    ]
    
    for path in paths:
        # PyCaret load_model ä¸éœ€è¦ .pkl å‰¯æª”å
        if os.path.exists(path + '.pkl'):
            try:
                return load_model(path)
            except:
                continue
    return None

model = load_prediction_model()
if not model:
    st.error("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆï¼Œè«‹ç¢ºèª `output/final_yield_prediction_model.pkl` å­˜åœ¨ã€‚")

# è¼‰å…¥ç‰¹å¾µæ¸…å–® (ç¢ºä¿è¼¸å…¥é †åºæ­£ç¢º)
required_features = [f'Sensor_{i}' for i in range(1, 11)] # é è¨­ fallback
try:
    if os.path.exists('required_features.pkl'):
        import pickle
        with open('required_features.pkl', 'rb') as f:
            required_features = pickle.load(f)
except Exception as e:
    pass # ä½¿ç”¨é è¨­å€¼

# --- 3. å»ºç«‹åˆ†é  ---
tab1, tab2, tab3 = st.tabs(["ğŸ” é æ¸¬èˆ‡é«˜é¢¨éšªæ¸…å–®", "ğŸ“Š æ¨¡å‹è§£é‡‹ (SHAP)", "ğŸ† æ¨¡å‹æ•ˆèƒ½å ±å‘Š"])

# === Tab 1: é æ¸¬åŠŸèƒ½ (å« Fail Ranking) ===
with tab1:
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("å–®é»æ¨¡æ“¬")
        input_data = {}
        # ç‚ºäº†æ¼”ç¤ºï¼Œåªé¡¯ç¤ºå‰ 5 å€‹ç‰¹å¾µ
        display_features = required_features[:5]
        for feature in display_features:
            val = st.number_input(f"{feature}", value=0.0)
            input_data[feature] = val
            
        predict_btn = st.button("ğŸš€ åŸ·è¡Œæ¨¡æ“¬é æ¸¬", type="primary")

        if predict_btn and model:
            # è£œé½Šå…¶ä»–ç‰¹å¾µç‚º 0
            for feature in required_features[5:]:
                input_data[feature] = 0.0
                
            df_input = pd.DataFrame([input_data])
            prediction = predict_model(model, data=df_input)
            
            # è™•ç† PyCaret 3.x è¼¸å‡º
            try:
                label = prediction['prediction_label'].iloc[0]
                score = prediction['prediction_score'].iloc[0]
                
                if label == 1:
                    st.error(f"âš ï¸ é æ¸¬çµæœ: **Fail (ç•°å¸¸)**")
                    st.metric("ç•°å¸¸æ©Ÿç‡", f"{score:.2%}")
                else:
                    st.success(f"âœ… é æ¸¬çµæœ: **Pass (æ­£å¸¸)**")
                    st.metric("å®‰å…¨ä¿¡å¿ƒ", f"{score:.2%}")
            except Exception as e:
                st.error(f"è§£æéŒ¯èª¤: {e}")

    with col2:
        st.subheader("ğŸ“‚ æ‰¹æ¬¡é æ¸¬ & Fail Ranking")
        st.info("ä¸Šå‚³ CSV æª”æ¡ˆï¼Œç³»çµ±å°‡è‡ªå‹•ç¯©é¸å‡º **é«˜é¢¨éšª (High Probability of Fail)** çš„æ™¶ç‰‡ã€‚")
        
        uploaded_file = st.file_uploader("ä¸Šå‚³æ¸¬è©¦æ•¸æ“š (CSV)", type="csv")
        if uploaded_file and model:
            try:
                batch_df = pd.read_csv(uploaded_file)
                predictions = predict_model(model, data=batch_df)
                
                # ç¢ºä¿æ¬„ä½åç¨±ä¸€è‡´
                lbl_col = 'prediction_label'
                score_col = 'prediction_score'
                
                if lbl_col in predictions.columns:
                    # --- é—œéµåŠŸèƒ½ï¼šFail Ranking ---
                    st.markdown("### ğŸ”¥ é«˜é¢¨éšªæ™¶ç‰‡æ’è¡Œæ¦œ (Top Failures)")
                    
                    # ç¯©é¸é æ¸¬ç‚º Fail (1) çš„è³‡æ–™
                    fail_df = predictions[predictions[lbl_col] == 1].copy()
                    
                    if not fail_df.empty:
                        # ä¾ç…§åˆ†æ•¸æ’åº (åˆ†æ•¸è¶Šé«˜ä»£è¡¨è¶Šåƒ Fail)
                        fail_df = fail_df.sort_values(by=score_col, ascending=False)
                        
                        # é¡¯ç¤ºå‰ 10 å
                        st.dataframe(
                            fail_df.head(10).style.background_gradient(subset=[score_col], cmap='Reds'),
                            use_container_width=True
                        )
                        st.warning(f"âš ï¸ å…±ç™¼ç¾ {len(fail_df)} å€‹æ½›åœ¨ç•°å¸¸æ™¶ç‰‡ï¼å»ºè­°å„ªå…ˆæª¢æŸ¥ä¸Šè¡¨ä¸­çš„é …ç›®ã€‚")
                    else:
                        st.success("ğŸ‰ å¤ªæ£’äº†ï¼æœ¬æ‰¹æ¬¡æ•¸æ“šä¸­æ²’æœ‰ç™¼ç¾é æ¸¬ç‚º Fail çš„æ™¶ç‰‡ã€‚")
                    
                    # ä¸‹è¼‰å®Œæ•´çµæœ
                    csv = predictions.to_csv(index=False).encode('utf-8')
                    st.download_button("ğŸ“¥ ä¸‹è¼‰å®Œæ•´é æ¸¬å ±å‘Š", csv, "predictions_with_ranking.csv", "text/csv")
                else:
                    st.error("é æ¸¬çµæœæ¬„ä½ä¸å¦‚é æœŸï¼Œç„¡æ³•ç”Ÿæˆæ’è¡Œæ¦œã€‚")
            except Exception as e:
                st.error(f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")

# === Tab 2: æ¨¡å‹è§£é‡‹ (SHAP) ===
with tab2:
    st.header("ğŸ§  SHAP æ¨¡å‹è§£é‡‹")
    # æ”¯æ´å¤šå€‹å¯èƒ½çš„åœ–ç‰‡è·¯å¾‘
    shap_paths = [
        'output/automl_reports/shap_summary_plot.png', # Step 1 å¯èƒ½ç”¢ç”Ÿçš„è·¯å¾‘
        'reports/SHAP Summary.png', 
        'output/shap_plots/shap_summary_plot.png'
    ]
    
    img_found = False
    for p in shap_paths:
        if os.path.exists(p):
            st.image(p, caption="Feature Importance (SHAP)", use_column_width=True)
            img_found = True
            break
            
    if not img_found:
        st.warning("âš ï¸ å°šæœªç”Ÿæˆ SHAP åœ–è¡¨ã€‚è«‹åŸ·è¡Œ `scripts/05_explain_model.py` æˆ–ç¢ºèªè·¯å¾‘ã€‚")

# === Tab 3: æ¨¡å‹æ•ˆèƒ½å ±å‘Š (æ•´åˆ Step 1 çµæœ) ===
with tab3:
    st.header("ğŸ† æ¨¡å‹æ•ˆèƒ½å„€è¡¨æ¿")
    
    # 1. æ¯”è¼ƒè¡¨æ ¼
    # Step 1 ç”Ÿæˆçš„æ˜¯ 'model_comparison_benchmark.csv'
    csv_path = 'reports/model_comparison_benchmark.csv'
    if os.path.exists(csv_path):
        st.subheader("æ¨¡å‹åŸºæº–æ¸¬è©¦ (Benchmark)")
        df_metrics = pd.read_csv(csv_path)
        # ç°¡å–®æ¸…ç†è¡¨æ ¼
        if 'Unnamed: 0' in df_metrics.columns:
            df_metrics = df_metrics.drop(columns=['Unnamed: 0'])
        st.dataframe(df_metrics.style.highlight_max(axis=0, color='lightgreen'))
    else:
        st.info("â„¹ï¸ å°šæœªæ‰¾åˆ°æ¨¡å‹æ¯”è¼ƒè¡¨ (model_comparison_benchmark.csv)ã€‚")

    # 2. åœ–è¡¨å±•ç¤º
    st.subheader("ğŸ“Š è¦–è¦ºåŒ–è©•ä¼°")
    
    # å®šç¾©åœ–è¡¨è·¯å¾‘ (æ ¹æ“š Step 1 çš„è¼¸å‡ºè¨­å®š)
    # Step 1 å­˜åˆ° output/automl_reports/
    img_dir = 'output/automl_reports' 
    
    col_a, col_b = st.columns(2)
    
    with col_a:
        st.markdown("**å­¸ç¿’æ›²ç·š (Learning Curve) - éæ“¬åˆæª¢æŸ¥**")
        p = os.path.join(img_dir, 'learning_curve.png')
        if os.path.exists(p):
            st.image(p, use_column_width=True)
        else:
            st.info("(å°šç„¡å­¸ç¿’æ›²ç·šåœ–)")
            
        st.markdown("**æ··æ·†çŸ©é™£ (Confusion Matrix)**")
        p = os.path.join(img_dir, 'confusion_matrix.png')
        if os.path.exists(p):
            st.image(p, use_column_width=True)
        else:
            st.info("(å°šç„¡æ··æ·†çŸ©é™£åœ–)")

    with col_b:
        st.markdown("**AUC æ›²ç·š (ROC Curve)**")
        p = os.path.join(img_dir, 'auc_roc_curve.png')
        if os.path.exists(p):
            st.image(p, use_column_width=True)
        else:
            st.info("(å°šç„¡ AUC åœ–)")

        st.markdown("**ç‰¹å¾µé‡è¦æ€§ (Feature Importance)**")
        p = os.path.join(img_dir, 'feature_importance.png')
        if os.path.exists(p):
            st.image(p, use_column_width=True)
        else:
            st.info("(å°šç„¡ç‰¹å¾µé‡è¦æ€§åœ–)")